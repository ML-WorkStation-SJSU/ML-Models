{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Random Forest\n",
    "* Supervised learning\n",
    "* Based on ensemble learning\n",
    "* Used for both **regression** and **classification problems**\n",
    "* General idea:  build multiple decision trees and aggregate them to get an accurate result\n",
    "\n",
    "Decision Tree\n",
    "* Deterministic: if the same data is given to it, the same tree will be produced each time.\n",
    "* Tend to overfit: Build the best tree with the given data, but may fail to generalize when unseen data is provided\n",
    "\n",
    "Random Forest performs better than Decision Tree because\n",
    "* random forest is built with random subset of the data (each decision tree in the forest may use different dataset, may also use different features)\n",
    "* random forest tends to minimized overfitting\n",
    "\n",
    "Reason why we want to use random sample dataset and random features\n",
    "*The idea of random forests is basically to build many decision trees (or other weak learners) that are decorrelated, so that their average is less prone to overfitting (reducing the variance). One way is subsampling of the training set. The reason why subsampling features can further decorrelate trees is, that if there are few dominating features, these features will be selected in many trees even for different subsamples, making the trees in the forest similar (correlated) again.* \n",
    "--- [stackoverflow anwser](https://datascience.stackexchange.com/questions/20304/why-do-we-pick-random-features-in-random-forest)\n",
    "\n",
    "![image](../../sample-data/random-forest-demo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrap sampling: random sampling with replacement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process\n",
    "\n",
    "1. Create a bootstrap sample with replacement, D training from x, y label these Xa, Ya\n",
    "2. Train the tree fa on Xa, Ya\n",
    "3. Average the predictions or take the majority vote to arrive at a final prediction\n",
    "\n",
    "### For regression problem\n",
    "<!-- ![regression average](../../sample-data/regre-average.png) -->\n",
    "<img src=\"../../sample-data/regre-average.png\" style=\"width:400px;height:auto\"/>\n",
    "Here, N is the total number of trees in the random forest. <br>\n",
    "a=1 represents the first tree in a forest, while the last tree in the forest is A. <br>\n",
    "$f_a(x)$ represents the prediction from a single tree.\n",
    "\n",
    "### For classification problem\n",
    "Majority voting or the most common answer is used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest in Scikit-learn\n",
    "\n",
    "For classification - RandomForestClassifier<br>\n",
    "For regression - RandomForestRegressor\n",
    "\n",
    "Parameters: \n",
    ">*Will add more with better understanding of the alogithm later* <br>\n",
    "\n",
    "* n_estimators: number of trees<br>\n",
    "* max_features: number-of-features to use<br>\n",
    "* bootstrap: if bootstrap samples (sampling subset of dataset)<br>\n",
    "* max_samples: If bootstrap if True, the number here will be used<br>\n",
    "* n_jobs: Support parallel processing (not sure how to use it yet though)\n",
    "* oob_score: This parameter is also known as out-of-the-bag sampling, and is a random forest cross-validation method. In this sampling method, about one-third of the data is not used to train the model and can be used to evaluate its performance. These samples are called the out-of-the-bag samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "* Ensemble Machine Learning Cookbook [link on Amazon](https://www.amazon.com/Ensemble-Machine-Learning-Cookbook-techniques/dp/1789136601)\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
