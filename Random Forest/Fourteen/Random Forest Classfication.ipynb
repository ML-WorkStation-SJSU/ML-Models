{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding a Decision Tree\n",
    "\n",
    "---\n",
    "\n",
    "A decision tree is the building block of a random forest and is an intuitive model. We can think of a decision tree as a series of yes/no questions asked about our data eventually leading to a predicted class (or continuous value in the case of regression).\n",
    "\n",
    "Decision tree tries to form nodes containing a high proportion of samples (data points) from a single class by finding values in the features that cleanly divide the data into classes.\n",
    "\n",
    "![](https://miro.medium.com/max/1170/0*dvVMJdNRzlUqOl2Z)\n",
    "![](https://miro.medium.com/max/4000/0*QwJ2oZssAQ2_cchJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Make a decision tree and train\n",
    "tree = DecisionTreeClassifier(random_state=RSEED)\n",
    "tree.fit(X, y)\n",
    "\n",
    "print(f'Model Accuracy: {tree.score(X, y)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gini Impurity\n",
    "\n",
    "---\n",
    "\n",
    "![](https://miro.medium.com/max/1514/1*mcHzG8OjhQ2ryiBH7MBPUA.png)\n",
    "\n",
    "The Gini Impurity of a node is the probability that a randomly chosen sample in a node would be incorrectly labeled if it was labeled by the distribution of samples in the node. For example, in the top (root) node, there is a 44.4% chance of incorrectly classifying a data point chosen at random based on the sample labels in the node\n",
    "\n",
    "![](https://miro.medium.com/max/2996/1*uAGS042OxMJ4Ic3k4s313Q.png)\n",
    "\n",
    "The weighted total Gini Impurity at each level of tree must decrease. At the second level of the tree, the total weighted Gini Impurity is 0.333:\n",
    "\n",
    "![](https://miro.medium.com/max/5658/1*gdMrk7yEPJLio0d0Sixtkg.png)\n",
    "\n",
    "**(The Gini Impurity of each node is weighted by the fraction of points from the parent node in that node.)**\n",
    "\n",
    "Eventually, the weighted total Gini Impurity of the last layer goes to 0 meaning each node is completely pure and there is no chance that a point randomly selected from that node would be misclassified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting: Or Why a Forest is better than One Tree\n",
    "\n",
    "---\n",
    "\n",
    "**The reason the decision tree is prone to overfitting when we don’t limit the maximum depth is because it has unlimited flexibility, meaning that it can keep growing until it has exactly one leaf node for every single observation, perfectly classifying all of them. If you go back to the image of the decision tree and limit the maximum depth to 2 (making only a single split), the classifications are no longer 100% correct. We have reduced the variance of the decision tree but at the cost of increasing the bias.**\n",
    "\n",
    "As an alternative to limiting the depth of the tree, which reduces variance (good) and increases bias (bad), we can combine many decision trees into a single ensemble model known as the random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "---\n",
    "\n",
    "The random forest combines hundreds or thousands of decision trees, trains each one on a slightly different set of the observations, splitting nodes in each tree considering a limited number of the features. The final predictions of the random forest are made by averaging the predictions of each individual tree.\n",
    "\n",
    "**So the prerequisites for random forest to perform well are:**\n",
    "\n",
    "- There needs to be some actual signal in our features so that models built using those features do better than random guessing.\n",
    "- The predictions (and therefore the errors) made by the individual trees need to have low correlations with each other.\n",
    "\n",
    "# Ensuring that the Models Diversify Each Other\n",
    "\n",
    "---\n",
    "\n",
    "- Random sampling of training data points when building trees：\n",
    "\n",
    "Bagging (Bootstrap Aggregation) — Decisions trees are very sensitive to the data they are trained on — small changes to the training set can result in significantly different tree structures.\n",
    "\n",
    "- Random subsets of features considered when splitting nodes\n",
    "\n",
    "The other main concept in the random forest is that only a subset of all the features are considered for splitting each node in each decision tree. Generally this is set to sqrt(n_features) for classification meaning that if there are 16 features, at each node in each tree, only 4 random features will be considered for splitting the node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest in Practice\n",
    "\n",
    "---\n",
    "\n",
    "See another file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OPtimization\n",
    "\n",
    "---\n",
    "\n",
    "Optimization refers to finding the best hyperparameters for a model on a given dataset. The best hyperparameters will vary between datasets, so we have to perform optimization (also called model tuning) separately on each datasets.\n",
    "\n",
    "- the number of decision trees\n",
    "- the maximum depth of each decision tree\n",
    "- the maximum number of features considered for splitting each node\n",
    "- the maximum number of data points required in a leaf node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias-variance tradeoff\n",
    "\n",
    "---\n",
    "\n",
    "a core issue in machine learning describing the balance between a model with high flexibility (high variance) that learns the training data very well at the cost of not being able to generalize to new data , and an inflexible model (high bias) that cannot learn the training data. A random forest reduces the variance of a single decision tree leading to better predictions on new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "- [https://towardsdatascience.com/understanding-random-forest-58381e0602d2](https://towardsdatascience.com/understanding-random-forest-58381e0602d2)\n",
    "\n",
    "- [https://towardsdatascience.com/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76](https://towardsdatascience.com/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python37364bitbasecondaabea366e711c4f00895702eb172cf1ab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
