{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting Ensemble Method\n",
    "\n",
    "Boosting is a general ensemble method that creates a **strong** classifier from a number of **weak** classifiers.\n",
    "\n",
    "This is done by building a model from the training data, then creating **a second model** that attempts to **correct the errors** from the first model. Models are **added until** the training set is predicted perfectly or a maximum number of models are added.\n",
    "\n",
    "## Cons\n",
    "\n",
    "**sensitive to “noise”, including:**\n",
    "- Mislabeled training data\n",
    "- Extraneous features\n",
    "- The issue with “noise” should become clear as we discuss the algorithm\n",
    "\n",
    "**May need LARGE number of classifiers**\n",
    "\n",
    "**In practice may not get wonderful results promised by the theory**\n",
    "\n",
    "## Pros\n",
    "\n",
    "- Weak (but nonrandom) classifiers can be combined into strong classifier\n",
    "- Easy and efficient to implement\n",
    "- Many different boosting algorithms: **AdaBoost** and **XgBoost**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost (From CS271)\n",
    "\n",
    "## Adaptive Boosting Ideas\n",
    "\n",
    "**At each iteration:**\n",
    "\n",
    "- Identify biggest remaining weakness\n",
    "- Determine which of available classifiers will help most wrt that weakness…\n",
    "- compute weight for new classifier\n",
    "\n",
    "this is a **greedy* approach!\n",
    "\n",
    "**AdaBoost is iterative and adaptive**\n",
    "\n",
    "- Make selection based on what has been selected so far, This is the sense that it is adaptive\n",
    "- we’ll always be greedy (Maybe make things worse)\n",
    "\n",
    "## Alogorithm\n",
    "\n",
    "### Input:\n",
    "\n",
    "![](./images/input.png)\n",
    "\n",
    "- We also have L classifiers (all weak), Denoted c1, c2, …, cL       \n",
    "- Each cj assigns a label to each Xi  \n",
    "- We combine cj to yield a classifier C(Xi)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterative process\n",
    "\n",
    "#### Generate a series of classifiers, call them C1(Xi), C2(Xi), …, CM(Xi), Where C(Xi) = CM(Xi) is final classifier\n",
    "\n",
    "- Cm(Xi) = α1k1(Xi) + α2k2(Xi) +…+ αmkm(Xi)\n",
    "- Cm(Xi) = Cm-1(Xi)  + αmkm(Xi)    \n",
    "- Each kj is one of the classifiers ci    \n",
    "- αi are weights  \n",
    "\n",
    "#### what we need to do at iteration j:\n",
    "\n",
    "- Which unused classifier kj = ci to select\n",
    "- Weight αj to assign to kj    \n",
    "\n",
    "#### exponential loss function\n",
    "\n",
    "![](./images/loss.png)\n",
    "\n",
    "we need to determine km and αm > 0\n",
    "\n",
    "### Solution\n",
    "\n",
    "![](./images/solution1.png)\n",
    "![](./images/solution2.png)\n",
    "![](./images/solution3.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of mth iteration\n",
    "\n",
    "- Select km so that number of errors, or misses (i.e., W2), is minimized\n",
    "- Once km is known, compute W2 and W\n",
    "- Computer αm as on previous slide\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost (From Someone)\n",
    "\n",
    "[Boosting and AdaBoost for Machine Learning](https://machinelearningmastery.com/boosting-and-adaboost-for-machine-learning/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 [0.28 0.42 0.14 0.16]\n",
      "100 [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1.]\n",
      "[1]\n",
      "0.983\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=4,\n",
    "                            n_informative=2, n_redundant=0,\n",
    "                            random_state=0, shuffle=False)\n",
    "\n",
    "clf = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "clf.fit(X, y)\n",
    "print(len(clf.feature_importances_), clf.feature_importances_)\n",
    "print(len(clf.estimator_weights_), clf.estimator_weights_)\n",
    "\n",
    "print(clf.predict([[0, 0, 0, 0]]))\n",
    "print(clf.score(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('anaconda3': virtualenv)",
   "language": "python",
   "name": "python37364bitanaconda3virtualenv28aa4b3b035c418190a6b962a95d21fb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
