{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayes’ Theorem is stated as:\n",
    "\n",
    "**P(h|d) = (P(d|h) * P(h)) / P(d)**\n",
    "\n",
    "Where:\n",
    "\n",
    "- **P(h|d)** is the probability of hypothesis h given the data d. This is called the posterior probability.\n",
    "- **P(d|h)** is the probability of data d given that the hypothesis h was true.\n",
    "- **P(h)** is the probability of hypothesis h being true (regardless of the data). This is called the prior probability of h.\n",
    "- **P(d)** is the probability of the data (regardless of the hypothesis).\n",
    "\n",
    "You can see that we are interested in calculating the posterior probability of P(h|d) from the prior probability p(h) with P(D) and P(d|h).\n",
    "\n",
    "---\n",
    "\n",
    "**After calculating the posterior probability for a number of different hypotheses, you can select the hypothesis with the highest probability**. This is the maximum probable hypothesis and may formally be called the **maximum a posteriori (MAP)** hypothesis.\n",
    "\n",
    "This can be written as:\n",
    "\n",
    "**MAP(h) = max(P(h|d))**\n",
    "\n",
    "or\n",
    "\n",
    "**MAP(h) = max((P(d|h) * P(h)) / P(d))**\n",
    "\n",
    "or\n",
    "\n",
    "**MAP(h) = max(P(d|h) * P(h))**\n",
    "\n",
    "The P(d) is a normalizing term which allows us to calculate the probability. We can drop it when we are interested in the most probable hypothesis as it is constant and only used to normalize.\n",
    "\n",
    "**When we want to figure out when X is set, which class for y should has a larger probability. The P(X) is same for different class y. So we can remove it**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "if we have an **even** number of instances in each class in our training data, then the **probability of each class (e.g. P(h)) will be equal**. Again, this would be a constant term in our equation and we could drop it so that we end up with:\n",
    "\n",
    "**MAP(h) = max(P(d|h))**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Types of Naive Bayes\n",
    "\n",
    "#### Multinomial Naive Bayes:\n",
    "\n",
    "This is mostly used for document classification problem, i.e whether a document belongs to the category of sports, politics, technology etc. The features/predictors used by the classifier are the frequency of the words present in the document.\n",
    "\n",
    "#### Bernoulli Naive Bayes:\n",
    "\n",
    "This is similar to the multinomial naive bayes but the predictors are boolean variables. The parameters that we use to predict the class variable take up only values yes or no, for example if a word occurs in the text or not.\n",
    "\n",
    "#### Gaussian Naive Bayes:\n",
    "\n",
    "When the predictors take up a continuous value and are not discrete, we assume that these values are sampled from a gaussian distribution.\n",
    "\n",
    "![](https://miro.medium.com/max/844/1*AYsUOvPkgxe3j1tEj2lQbg.gif)\n",
    "\n",
    "Since the way the values are present in the dataset changes, the formula for conditional probability changes to\n",
    "\n",
    "![](https://miro.medium.com/max/1576/1*0If5Mey7FnW_RktMM5BkaQ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Naive Bayes:\n",
    "\n",
    "Naive Bayes can be extended to **real-valued** attributes, most commonly by **assuming a Gaussian distribution.**\n",
    "\n",
    "This extension of naive Bayes is called Gaussian Naive Bayes. Other functions can be used to estimate the distribution of the data, but the Gaussian (or Normal distribution) is the easiest to work with because you only need to estimate the mean and the standard deviation from your training data.\n",
    "\n",
    "## Representation for Gaussian Naive Bayes\n",
    "\n",
    "Above, we calculated the probabilities for input values for each class using a frequency. With real-valued inputs, we can calculate the mean and standard deviation of input values (x) for each class to summarize the distribution.\n",
    "\n",
    "This means that in addition to the probabilities for each class, we must also store the mean and standard deviations for each input variable for each class.\n",
    "\n",
    "Learn a Gaussian Naive Bayes Model From Data\n",
    "This is as simple as calculating the mean and standard deviation values of each input variable (x) for each class value.\n",
    "\n",
    "**mean(x) = 1/n * sum(x)**\n",
    "\n",
    "Where n is the number of instances and x are the values for an input variable in your training data.\n",
    "\n",
    "We can calculate the standard deviation using the following equation:\n",
    "\n",
    "**standard deviation(x) = sqrt(1/n * sum(xi-mean(x)^2 ))**\n",
    "\n",
    "This is the square root of the average squared difference of each value of x from the mean value of x, where n is the number of instances, sqrt() is the square root function, sum() is the sum function, xi is a specific value of the x variable for the i’th instance and mean(x) is described above, and ^2 is the square.\n",
    "\n",
    "## Make Predictions With a Gaussian Naive Bayes Model\n",
    "\n",
    "Probabilities of new x values are calculated using the Gaussian Probability Density Function (PDF).\n",
    "\n",
    "When making predictions these parameters can be plugged into the Gaussian PDF with a new input for the variable, and in return the Gaussian PDF will provide an estimate of the probability of that new input value for that class.\n",
    "\n",
    "**pdf(x, mean, sd) = (1 / (sqrt(2 * PI) * sd)) * exp(-((x-mean^2)/(2*sd^2)))**\n",
    "\n",
    "Where pdf(x) is the Gaussian PDF, sqrt() is the square root, mean and sd are the mean and standard deviation calculated above, PI is the numerical constant, exp() is the numerical constant e or Euler’s number raised to power and x is the input value for the input variable.\n",
    "\n",
    "We can then plug in the probabilities into the equation above to make predictions with real-valued inputs.\n",
    "\n",
    "For example, adapting one of the above calculations with numerical values for weather and car:\n",
    "\n",
    "**go-out = P(pdf(weather)|class=go-out) * P(pdf(car)|class=go-out) * P(class=go-out)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Prepare Your Data For Naive Bayes\n",
    "\n",
    "#### Categorical Inputs: \n",
    "Naive Bayes assumes label attributes such as binary, categorical or nominal.\n",
    "\n",
    "#### Gaussian Inputs: \n",
    "If the input variables are real-valued, a Gaussian distribution is assumed. In which case the algorithm will perform better if the univariate distributions of your data are Gaussian or near-Gaussian. This may require removing outliers (e.g. values that are more than 3 or 4 standard deviations from the mean).\n",
    "\n",
    "#### Classification Problems: \n",
    "Naive Bayes is a classification algorithm suitable for binary and multiclass classification.\n",
    "\n",
    "#### Log Probabilities: \n",
    "The calculation of the likelihood of different class values involves multiplying a lot of small numbers together. This can lead to an underflow of numerical precision. As such it is good practice to use a log transform of the probabilities to avoid this underflow.\n",
    "\n",
    "#### Kernel Functions: \n",
    "Rather than assuming a Gaussian distribution for numerical input values, more complex distributions can be used such as a variety of kernel density functions.\n",
    "\n",
    "#### Update Probabilities: \n",
    "When new data becomes available, you can simply update the probabilities of your model. This can be helpful if the data changes frequently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "\n",
    "[Naive Bayes for Machine Learning](https://machinelearningmastery.com/naive-bayes-for-machine-learning/)\n",
    "\n",
    "[Naive Bayes Classifier From Scratch in Python](https://machinelearningmastery.com/naive-bayes-classifier-scratch-python/)\n",
    "\n",
    "[Naive Bayes Classifier](https://towardsdatascience.com/naive-bayes-classifier-81d512f50a7c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes model accuracy(in %): 95.0\n"
     ]
    }
   ],
   "source": [
    "# load the iris dataset \n",
    "from sklearn.datasets import load_iris \n",
    "iris = load_iris() \n",
    "  \n",
    "# store the feature matrix (X) and response vector (y) \n",
    "X = iris.data \n",
    "y = iris.target \n",
    "  \n",
    "# splitting X and y into training and testing sets \n",
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=1) \n",
    "  \n",
    "# training the model on training set \n",
    "from sklearn.naive_bayes import GaussianNB \n",
    "gnb = GaussianNB() \n",
    "gnb.fit(X_train, y_train) \n",
    "  \n",
    "# making predictions on the testing set \n",
    "y_pred = gnb.predict(X_test) \n",
    "  \n",
    "# comparing actual response values (y_test) with predicted response values (y_pred) \n",
    "from sklearn import metrics \n",
    "print(\"Gaussian Naive Bayes model accuracy(in %):\", metrics.accuracy_score(y_test, y_pred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
